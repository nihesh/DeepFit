# Author : Nihesh Anderson 
# Date   : 13 Feb, 2020
# File 	 : Train.py

"""
[ENTRY - NN Training] Training script for Deep Robust Multi-Model Fitting
"""

from src.constants import DATASET_TYPES

# Script Arguments

# Dataset Arguments
DATA_ROOT = "./data/processed/SyntheticHomography"
TYPE = "Homography"
NUM_SAMPLES = 10000									

# DGSAC Arguments
TOPK = 60
NUM_HYPOTHESIS = 100

# Training Arugments
FEATURE_MAP_SCALE = 4
BATCH_SIZE = 5
NUM_WORKERS = 0
LEARNING_RATE = 0.001
EPOCHS = 100
OUTPUT_HYP_CNT = 1000

# Results
SAVE_PATH = "./results/"

import src.dataset as dataset
from src.constants import SEED
import numpy as np
import cv2
from matplotlib import pyplot as plt
import torch
import src.geometry as geometry
import os
import torch.optim as optim
import models.deepfit as deepfit
import random
import src.utils as utils
import src.loss as loss

def ResetWorkspace():

	"""
	A start up function to prepare the simulation environment
	Tasks achieved:
		* Set seed value for RNG
		* Empty results folder
	"""

	random.seed(SEED)
	np.random.seed(SEED)
	torch.manual_seed(SEED)
	torch.backends.cudnn.deterministic = True
	torch.backends.cudnn.benchmark = False

	os.system("rm -rf ./results")
	os.makedirs(SAVE_PATH)

if(__name__ == "__main__"):

	ResetWorkspace()
	
	dataset = dataset.DataReader(DATA_ROOT, TYPE, NUM_SAMPLES, TOPK, NUM_HYPOTHESIS)
	dataloader = torch.utils.data.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS)

	# Visualise Density/Residual features for hand crafted feature space
	data, encoding, label = dataset[1]
	utils.VisualiseEmbedding(
			data.cpu().detach().numpy(), 
			encoding.cpu().detach().numpy(), 
			label.cpu().detach().numpy(), 
			SAVE_PATH + "cluster_visualisation.png"
		)

	model = deepfit.DeepFit(NUM_HYPOTHESIS, OUTPUT_HYP_CNT, FEATURE_MAP_SCALE, data.shape[0]).cuda()
	optimiser = optim.Adam(model.parameters(), lr = LEARNING_RATE)

	# Train the model over multiple epochs - assumes the number of correspondences over all the samples in a batch is constant or fixed
	for epoch in range(EPOCHS):

		# Book keeping for different losses
		total_cnt = 0
		tot_loss = 0
		tot_dist_align_loss = 0

		for data, encoding, label in dataloader:

			batch_size = data.shape[0]
			num_correspondences = data.shape[1]

			true_distribution, true_state, num_structures = utils.compute_silhouette_order(data, label)
			structure_id = 0
			max_structures = len(true_distribution)	

			# Initialise losses
			dist_align_loss = 0
			
			# Create an empty initial state vector
			state = torch.zeros([batch_size, num_correspondences]).float().cuda()

			# Train as long as there are at least 2 datasets with the current structure we are trying to fit - pointnet's batch norm won't work otherwise
			# It's ok if we skip fitting a structure for a dataset - It will get trained in subsequent iterations
			while(structure_id < max_structures and true_distribution[structure_id].shape[0] > 1):

				# Identify all datasets with structure_id + 1 number of structures
				cur_encoding = encoding[np.argwhere(num_structures >= (structure_id + 1)).reshape(-1)]
				cur_state = state[np.argwhere(num_structures >= (structure_id + 1)).reshape(-1)]
				cur_batch_size = cur_encoding.shape[0]

				# HACK - Replace this
				cur_state = true_state[structure_id]

				# Generate embedding from pointnet
				embedding, distribution = model(cur_encoding, cur_state)

				print(distribution[0][:20])
				print(true_distribution[structure_id][0][:20])

				bce = loss.dist_align_bce(true_distribution[structure_id], distribution)
				dist_align_loss += bce * cur_batch_size

				total_cnt += cur_batch_size
				structure_id += 1

			loss_now = dist_align_loss

			# Back propagate
			model.zero_grad()
			loss_now.backward()
			optimiser.step()

			tot_loss += loss_now
			tot_dist_align_loss += dist_align_loss

			print("EPOCH {epoch} | Total Loss: {loss} | BCE Loss: {bce}".format(
					epoch = epoch,
					loss = tot_loss.item() / total_cnt,
					bce = tot_dist_align_loss.item() / total_cnt
				))

		# Visualise embedding space learnt by pointnet and the coorresponding encoding space generated by dgsac
		utils.VisualiseEmbedding(
				data[0].cpu().detach().numpy(), 
				encoding[0].cpu().detach().numpy(), 
				label[0].cpu().detach().numpy(), 
				SAVE_PATH + "cluster_visualisation.png"
			)

		utils.VisualiseEmbedding(
			data[0].cpu().detach().numpy(), 
			embedding[0].cpu().detach().numpy(), 
			label[0].cpu().detach().numpy(), 
			SAVE_PATH + "cluster_visualisation2.png"
		)